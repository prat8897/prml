---
layout: page-sidenav
group: "Chapter. 1"
title: "1. Example: Polynomial Curve Fitting"
---
- Suppose that we are given a training set comprising $N$ observations of $x$, written $x ≡ (x_{1} , . . . , x_{N} )^T$ together with corresponding observations of the values of **t**, denoted as $t≡(t_{1},...,t_{N})^T$

![figure1.2]({{ site.baseurl }}/images/Figure1.2.png){:class="center-block" height="200px"}

- Figure 1.2 shows a plot of a training set comprising of N = 10 data points. The input data set $x$ was generated by choosing values of $x_{n}$, for $n = 1,...,N$ spaced uniformly in range $[0,1]$ and the target data set $t$ was obtained by first computing the corresponding values of the function $sin(2πx)$ and then adding a small level of random noise having a Gaussian distribution to each such point in order to obtain the corresponding value $t_{n}$.
- Our goal is to exploit this training set in order to make predictions of the value $\hat{t}$ of the target variable for some new value $\hat{x}$ of the input variable.
- This involves implicitly trying to discover the underlying function $sin(2πx)$
- Curve Fitting: We shall now fit the data using a polynomial function of the form $$y(x,{\bf w})=w_0+w_1x+w_2x^2+...+w_Mx^M=\sum_{j=0}^{M}w_jx^{\;j} \qquad{(1.1)}$$ Where $$ M = \textrm{Order of the Polynomial} \\ {\bf w} ≡  (w_{0},...,w_{M})$$

>Note: although the polynomial function $y(x, {\bf w})$ is a nonlinear function of $x$, it is a linear function of the coefficients ${\bf w}$. This is known as a *linear model*.

- The values of the coefficients will be determined by fitting the polynomial to the training data. This can be done by minimizing an **error function** that measures the misfit between the function $y(x, {\bf w})$, for any given value of ${\bf w}$, and the training set data points.
- Let us define an error function $E({\bf w})$ such that we minimize the sum of the squares of the errors between the predictions $y(x_{n}, {\bf w})$ for each data point $x_{n}$ and the corresponding values $t_{n}$:$$E({\bf w})=\dfrac{1}{2}\sum_{n=1}^{N}\{y(x_n,{\bf w})-t_n\}^2 \qquad{(1.2)}$$ where the factor $\frac{1}{2}$ is included for later convenience.

![figure1.3]({{ site.baseurl }}/images/Figure1.3.png){:class="center-block" height="200px"}

- The error function $(1.2)$ corresponds to (one half of) the sum of the squares of the displacements (shown by the vertical green bars) of each data point from the function $y(x, {\bf w})$.

> $E({\bf w}) = 0 \iff y(x_n,{\bf w})-t_n = 0, \forall x_n, t_n$

<div class="text-center">
  <img src="{{ site.baseurl }}/images/Figure1.4a.png" alt="Figure 1.4a" height="180px" />
  <img src="{{ site.baseurl }}/images/Figure1.4b.png" alt="Figure 1.4b" height="180px" />
</div>
<div class="text-center">
  <img src="{{ site.baseurl }}/images/Figure1.4c.png" alt="Figure 1.4c" height="180px" />
  <img src="{{ site.baseurl }}/images/Figure1.4d.png" alt="Figure 1.4d" height="180px" />
</div>

- Plots of polynomials having various orders $M$, shown as red curves, fitted to the data set shown in Figure 1.2.

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTI5Njc1NjAxLC05ODU5NzUxMzksLTIyMj
EzNjI3MywyMTE0OTc5MzgzLDE2Mjg3MjM1MzIsMTM5Njk1Nzc4
OSwtMjAwNzAxNTc5NV19
-->