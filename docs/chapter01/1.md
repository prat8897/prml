---
layout: page-sidenav
group: "Chapter. 1"
title: "1. Example: Polynomial Curve Fitting"
---
- Suppose that we are given a training set comprising $N$ observations of $x$, written $$x ≡ (x_{1} , . . . , x_{N} )^T$$ together with corresponding observations of the values of **t**, denoted
$$t≡(t_{1},...,t_{N})^T$$

! [figure1.2] ({{site.baseurl}} / images / Figure1.2.png) {: class = "center-block" height = "200px"}

- Figure1.2 shows a plot of a training set comprising of N = 10 data points. The input data set $x$ was generated by choosing values of $x_{n}$, for $n = 1,...,N$ spaced uniformly in range $[0,1]$ and the target data set $t$ was obtained by first computing the corresponding values of the function $sin(2πx)$ and then adding a small level of random noise having a Gaussian distribution to each such point in order to obtain the corresponding value $t_{n}$.
- Our goal is to exploit this training set in order to make predictions of the value $\hat{t}$ of the target variable for some new value $\hat{x}$ of the input variable.
- This involves implicitly trying to discover the underlying function $sin(2πx)$
- Curve Fitting: We shall now fit the data using a polynomial function of the form $$y(x,{\bf w})=w_0+w_1x+w_2x^2+...+w_Mx^M=\sum_{j=0}^{M}w_jx^{\;j} \qquad{(1.1)}$$

Where $$ M = \textrm{Order of the Polynomial} \\ n - whola $$
 > ** Target **: After observing the input variable \\ (x \\) in real range, we want to predict the value of target \\ (t \\) in real range based on this observation.
 Begin by assuming a model to use as an example. Suppose target \\ (t \\) is a real value derived from function \\ (\ sin (2 \ pi x) \\). -The expressed value at this time contains noise. The noise generated here may be because a given problem is inherently a probabilistic process, -Or may be included in observations for other reasons not observed.
 ! [figure1.2] ({{site.baseurl}} / images / Figure1.2.png) {: class = "center-block" height = "200px"}
 In the figure, the green line is called the \\ (\ sin (2 \ pi x) \\) function. In this function, a sample generated by a Gaussian random distribution is represented by a blue circle. These samples were all expressed with noise. For the sake of understanding, the sample probably appears to be taken at regular intervals. It is not easy to accurately obtain the original sample generation function \\ (\ sin (2 \ pi x) \\) using observation data including noise. We consider this using two theories: ** Probability Theory ** provides a mathematical framework for quantifying and expressing uncertainty. ** Decision Theory **: Provides a methodology to perform optimal prediction based on probabilistic representation based on appropriate criteria. Let us be more specific about what we need to do. After approximating the model using the training data, the prediction result \\ (\ widehat {t} \\) should be provided for any input data \\ (\ widehat {x} \\). We choose the model in the following way.
 $$ y (x, {\ bf w}) = w_0 + w_1x + w_2x ^ 2 + ... + w_Mx ^ M = \ sum_ {j = 0} ^ {M} w_jx ^ {\; j} \ qquad { (1.1)} $$
 This is a polynomial model. Why did polynomial models suddenly emerge? -If we assume that the function to be modeled is not known yet, we need an approximation to approximate it -In general, many functions are approximated by Taylor series or Fourier transform. Think of it as performing an approximate equation in the form of a Taylor series. Introduce a polynomial function as an example of the Taylor series. Usually we use Taylor series as a means to approximate a function at a specific location. A typical Taylor series can produce an approximation that fits better at a particular location by increasing the order of \\ (M \\). -But it may be slightly different from what we want to do,  We aim to make the model as robust as possible by generalizing it. What this means is that you are not creating a model that fits only the given sample data, -We need a model that gives plausible target values ​​for any data provided after model construction.
 Anyway, once you decide to use the model as a polynomial, The problem now turns to the problem of finding the best \\ ({\ bf w} \\) within a fixed \\ (M \\) value. We don't need to assume that the original \\ (M \\) values ​​are fixed, but here we start. Let's see how to find the optimal coefficient \\ ({\ bf w} \\) to approximate the equation. Usually, when solving such a problem, an error function is introduced to solve the problem. \\ ({\ bf w} by minimizing the difference between the \\ (y (x, {\ bf w}) \\) function derived from the model and the actual target value \\ (t \\) \\) value can be determined. And the error function usually uses the sum-of-squares error function. Why do we use the sum of squares of displacements rather than the sum of displacements as an error function? -If the \\ (y \\) function satisfies `convex`, the error function also becomes a function that satisfies` convex` and becomes differentiable A simple derivative of the error function over \\ ({\ bf w} \\) gives the only solution to the minimization problem. The concept of `convex` is mentioned later, so let's think of it as a quadratic equation. In the graph of quadratic equations, there is only one minimum or maximum value. Also, the reason that the formula contains constant coefficients such as \\ (\ frac {1} {2} \\) is to ease the development of subsequent mathematical calculations.
 $$ E ({\ bf w}) = \ dfrac {1} {2} \ sum_ {n = 1} ^ {N} \ {y (x_n, {\ bf w})-t_n \} ^ 2 \ qquad {(1.2)} $$
 ! [figure1.3] ({{site.baseurl}} / images / Figure1.3.png) {: class = "center-block" height = "200px"}
 Now we can think of the function approximation as a problem of finding \\ ({\ bf w} \\) which minimizes the value of \\ (E ({\ bf w}) \\). Since the defined error function is a quadratic function for \\ ({\ bf w} \\), the value that minimizes it is guaranteed to have a unique solution. The term `quadratic` is a term often used in linear algebra. -Simply put, a polynomial of degree only. This will be discussed in more detail later. Let \\ ({\ bf w} \\) be the value of \\ ({\ bf w} ^ * \\) for a unique solution. -The function value obtained at this time is \\ (y (x, {\ bf w} ^ *) \\). -The original \\ (y \\) function is a function for \\ (x \\) and \\ ({\ bf w} \\) but with training data \\ ({\ bf w} \\) Can be fixed to a specific value \\ ({\ bf w} ^ * \\), The function \\ (y \\) can be thought of as a function that is only processed for a single variable \\ (x \\). -Don't worry if you don't know. We will continue to discuss this in the future.
 -----
 Next, consider the number of orders \\ (M \\) in \\ (x \\) of the polynomial function (* polynomial *). If you can determine the value \\ (M \\) directly from the beginning, you have to worry about which value to choose. This is an important concept called * model comparison * or * model selection *. If the degree of polynomial is different, the function of the predicted model is completely different. After all, if the number of \\ (M \\) is different, you can think of them as different models.
     Selecting the best model for a given data also means that it is of interest. -I'll talk more about it in Chapter 2, so let's just touch on the concepts.
 <div class = "text-center"> <img src = "{{site.baseurl}} / images / Figure1.4a.png" alt = "Figure 1.4a" height = "180px" /> <img src = "{{site.baseurl}} / images / Figure1.4b.png" alt = "Figure 1.4b" height = "180px" /> </ div> <div class = "text-center"> <img src = "{{site.baseurl}} / images / Figure1.4c.png" alt = "Figure 1.4c" height = "180px" /> <img src = "{{site.baseurl}} / images / Figure1.4d.png" alt = "Figure 1.4d" height = "180px" /> </ div>
 As can be seen from the figure, if the value of \\ (M \\) is too small, under-fitting occurs. If it is too high, over-fitting occurs. -If \\ (M \\) is 1, it is just a straight line equation, which is a simple linear regression equation. If \\ (M \\) is too large, the learning about noise is also included in the model itself, resulting in an overfit of the given data. -Look at the picture. At present, the value of M seems to be the best at around 3. One might wonder if the model with \\ (M = 9 \\) is the better one for the current given data. However, our final goal is to create a ** generalized ** model that gives the best results when new data \\ (x \\) is entered. -Therefore, the noise information should be composed of the model excluded as much as possible.
 ----
 Now let's evaluate each model. Let's define the error \\ (RMS \\) to quantify the magnitude of the error value for each model.
 $$ E_ {RMS} = \ sqrt {\ frac {2E ({\ bf w} ^ *)} {N}} \ qquad {(1.3)} $$
 -Since the problem definition itself is a model that minimizes the error \\ (E \\), the smaller the \\ (RMS \\) value is, the better the model is. Why divide the \\ (RMS \\) value by the sample size \\ (N \\)? A normalization factor for correcting a scale problem that occurs when having different data sizes.
 ! [figure1.5] ({{site.baseurl}} / images / Figure1.5.png) {: class = "center-block" height = "250px"}
 -By checking the value of \\ (RMS \\) for each \\ (M \\), it is easy to see the difference between the training data and the test data. -From the value where \\ (M \\) is \\ (9 \\), the error value of the test data increases rapidly. Of course, the learning data \\ (RMS \\) keeps getting smaller. This is the point where overfitting occurs. -You can see how the \\ (y (x, {\ bf w} ^ *) \\) function behaves. (See the picture when \\ (M = 9 \\)) -We can get some intuition from this. -If you check the value of \\ ({\ bf w} \\) obtained from the polynomial function, each \\ ({\ bf w} \ with \\ (M \\) is \\ (9 \\) You can see that the values ​​are very large or very small. This is due to an extreme attempt to calibrate each \\ ({\ bf w} \\) value in order to move the model function as close as possible around the maximally expressed data. Although not always the case, the parameter values ​​when overfitted show a large deviation from each other.
 -----
 Next, consider the results of the model according to the size of the data.
 <div class = "text-center"> <img src = "{{site.baseurl}} / images / Figure1.6a.png" alt = "Figure 1.6a" height = "200px" /> <img src = "{{site.baseurl}} / images / Figure1.6b.png" alt = "Figure 1.6b" height = "200px" /> </ div>
 -The values ​​of \\ (M \\) used in the above figure are both 9 and the same. (\\ (M = 9 \\)) -On the left, however, there are 15 sample sizes, and on the right, 100 sample sizes. As we saw earlier, overfitting occurred at \\ (M = 9 \\) when using a small number of samples. (left) However, this is not the case even though the right side with large data size is \\ (M = 9 \\). -First of all, the larger the number (observation) of observation data, the more the overfit phenomenon can be prevented. -From a heuristic point of view, the number of model parameters should be set to less than 1/5 or 1/10 of the sample size. Of course, it is reasonable to decide the complexity of the model according to the problem to be solved. -Over-fitting phenomenon is inevitable Destiny. We'll look at this problem further with MLE (Maximum likelihood estimation). We will also look at solving this problem through a Bayesian approach. In addition, we will see how the Bayesian approach determines the effective number of model parameters. -But for now, let's learn some basics and proceed.
 Given little data, is there a way to create a model with as high order parameters as possible? With some techniques, you can try to increase the complexity of the model and to avoid overfitting as much as possible.  This is the * regularization * technique. -As we saw in the previous problem, we can see that when overfitting occurs, certain \\ ({\ bf w} \\) values ​​get very large or very small. Of course, this is a phenomenon seen in regression problems.  However, in all models, overfitting does not make the \\ ({\ bf w} \\) value very large or small. -Let's think about these things later, but first let's focus on the issues that are given right now. Thus, it restricts the range of values ​​that \\ ({\ bf w} \\) can take to prevent the appearance of \\ ({\ bf w} \\) with the largest possible value. -If this is formulated and introduced into the error function, it can be described as follows. (Why this form of expression appeared later on.)
 $$ \ tilde {E} ({\ bf w}) = \ dfrac {1} {2} \ sum_ {n = 1} ^ {N} \ {y (x_n, {\ bf w})-t_n \} ^ 2 + \ dfrac {\ lambda} {2} \ | {\ bf w} \ | ^ 2 \ qquad {(1.4)} $$
 -Where \\ (\\ | {\ bf w} \\ | ^ 2 \\; {\ equiv} \\; {\ bf w} ^ T {\ bf w} \\; {\ equiv} \\; w_0 ^ 2 + w_1 ^ 2 + ... + w_M ^ 2 \\) The coefficient \\ (\ lambda \\) is a very important factor and is called the regularization term * or * regularization coefficient *. \\ (\\ | {\ bf w} \\ | ^ 2 \\) is called a * regularizer *. \\ (w_0 \\) is sometimes omitted. -Even with the addition of the regular function, the expression itself remains the closed form for \\ (w \\). You don't have to be too burdened with the term closed structure, just understand that you can still get a clear solution. This form of normalization is called * Ridge * normalization. For \\ ({\ bf w} \\), the regularization element is of the form `quadratic`. There are many other variants, including * Lasso *. It is mentioned again in Chapter 3, so let's take a closer look. In fact, this approach has long been known as * weight decay * in the area of ​​machine learning.
 -----
 Let's take a closer look at the normalization coefficients.
 <div class = "text-center"> <img src = "{{site.baseurl}} / images / Figure1.7a.png" alt = "Figure 1.7a" height = "200px" /> <img src = "{{site.baseurl}} / images / Figure1.7b.png" alt = "Figure 1.7b" height = "200px" /> </ div>
 -Let's see what happens if you add \\ (\ lambda \\) to the result of applying regularization to the result of applying \\ (M = 9 \\) and the same sample data. -If \\ (\ ln {\ lambda} =-18 \\), the overfit is eliminated and you get an expression very close to the original \\ (\ sin (2 {\ pi} x) \\) Can be.  However, as you can see from \\ (\ ln {\ lambda} = 0 \\), an overly large \\ (\ lambda \\) value again produces an under-fitting phenomenon.
 ! [figure1.8] ({{site.baseurl}} / images / Figure1.8.png) {: class = "center-block" height = "220px"}
 Now let's look again at the \\ (RMS \\) result with the addition of the normalization factor. -Unlike the previous results, we can see that it shows good performance on both training data and test data.
 <!-stackedit_data: eyJoaXN0b3J5IjpbLTIzNzcyNTk5Nl19 ->
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE3NDg1MDUzMzYsMTYyODcyMzUzMiwxMz
k2OTU3Nzg5LC0yMDA3MDE1Nzk1XX0=
-->